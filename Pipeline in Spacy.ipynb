{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa4a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26978816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44df2861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfe5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a716f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e335b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x11478c32fa0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x11478887640>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x114784a5820>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x114784c1c80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x11478356900>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x114784a5cf0>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f025364",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd222ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  proper noun  |  Captain\n",
      "america  |  proper noun  |  america\n",
      "ate  |  verb  |  eat\n",
      "100  |  numeral  |  100\n",
      "$  |  noun  |  $\n",
      "of  |  adposition  |  of\n",
      "samosa  |  proper noun  |  samosa\n",
      ".  |  punctuation  |  .\n",
      "Then  |  adverb  |  then\n",
      "he  |  pronoun  |  he\n",
      "said  |  verb  |  say\n",
      "I  |  pronoun  |  I\n",
      "can  |  auxiliary  |  can\n",
      "do  |  verb  |  do\n",
      "this  |  pronoun  |  this\n",
      "all  |  determiner  |  all\n",
      "day  |  noun  |  day\n",
      ".  |  punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53550664",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48ab8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7393ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8b66c",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c85da62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96cadbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5091dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ravi  |  noun  |  ravi\n",
      "and  |  coordinating conjunction  |  and\n",
      "Raju  |  proper noun  |  Raju\n",
      "are  |  auxiliary  |  be\n",
      "the  |  determiner  |  the\n",
      "best  |  adjective  |  good\n",
      "friends  |  noun  |  friend\n",
      "from  |  adposition  |  from\n",
      "school  |  noun  |  school\n",
      "days  |  noun  |  day\n",
      ".  |  punctuation  |  .\n",
      "They  |  pronoun  |  they\n",
      "wanted  |  verb  |  want\n",
      "to  |  particle  |  to\n",
      "go  |  verb  |  go\n",
      "for  |  adposition  |  for\n",
      "a  |  determiner  |  a\n",
      "world  |  noun  |  world\n",
      "tour  |  noun  |  tour\n",
      "and  |  coordinating conjunction  |  and\n",
      "\n",
      "  |  space  |  \n",
      "\n",
      "visit  |  verb  |  visit\n",
      "famous  |  adjective  |  famous\n",
      "cities  |  noun  |  city\n",
      "like  |  adposition  |  like\n",
      "Paris  |  proper noun  |  Paris\n",
      ",  |  punctuation  |  ,\n",
      "London  |  proper noun  |  London\n",
      ",  |  punctuation  |  ,\n",
      "Dubai  |  proper noun  |  Dubai\n",
      ",  |  punctuation  |  ,\n",
      "Rome  |  proper noun  |  Rome\n",
      "etc  |  other  |  etc\n",
      "and  |  coordinating conjunction  |  and\n",
      "also  |  adverb  |  also\n",
      "they  |  pronoun  |  they\n",
      "called  |  verb  |  call\n",
      "their  |  pronoun  |  their\n",
      "another  |  determiner  |  another\n",
      "friend  |  noun  |  friend\n",
      "Mohan  |  proper noun  |  Mohan\n",
      "to  |  particle  |  to\n",
      "take  |  verb  |  take\n",
      "part  |  noun  |  part\n",
      "of  |  adposition  |  of\n",
      "this  |  determiner  |  this\n",
      "world  |  noun  |  world\n",
      "tour  |  noun  |  tour\n",
      ".  |  punctuation  |  .\n",
      "\n",
      "  |  space  |  \n",
      "\n",
      "They  |  pronoun  |  they\n",
      "started  |  verb  |  start\n",
      "their  |  pronoun  |  their\n",
      "journey  |  noun  |  journey\n",
      "from  |  adposition  |  from\n",
      "Hyderabad  |  proper noun  |  Hyderabad\n",
      "and  |  coordinating conjunction  |  and\n",
      "spent  |  verb  |  spend\n",
      "next  |  adjective  |  next\n",
      "3  |  numeral  |  3\n",
      "months  |  noun  |  month\n",
      "travelling  |  verb  |  travel\n",
      "all  |  determiner  |  all\n",
      "the  |  determiner  |  the\n",
      "wonderful  |  adjective  |  wonderful\n",
      "cities  |  noun  |  city\n",
      "in  |  adposition  |  in\n",
      "the  |  determiner  |  the\n",
      "world  |  noun  |  world\n",
      "and  |  coordinating conjunction  |  and\n",
      "cherish  |  verb  |  cherish\n",
      "a  |  determiner  |  a\n",
      "happy  |  adjective  |  happy\n",
      "moments  |  noun  |  moment\n",
      "!  |  punctuation  |  !\n",
      "\n",
      "  |  space  |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcf4479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raju\n",
      "Paris\n",
      "London\n",
      "Dubai\n",
      "Rome\n",
      "Mohan\n",
      "Hyderabad\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for token in doc:\n",
    "    if spacy.explain(token.pos_) == \"proper noun\":\n",
    "        count += 1\n",
    "        print(token) \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "434670ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4323a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a22cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "Walmart\n",
      "Amazon\n",
      "Microsoft\n",
      "Google\n",
      "Infosys, Reliance\n",
      "HDFC Bank\n",
      "Hindustan Unilever\n",
      "Bharti Airtel\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0130e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
